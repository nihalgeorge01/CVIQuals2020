{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6NEcxwh0Ej7T"
   },
   "source": [
    "# `Hit the Gym: MiniProject for RL Games CPP`\n",
    "\n",
    "The exercises in this notebook are meant for CVI aspirants who wish to work on the RL Games CPP. \n",
    "\n",
    "**About the project:** The goal of the RL Games project is to explore and analyse current Reinforcement Learning Methods by deploying them on Atari games, Minesweeper, slither.io etc. Further, the techniques developed can be used to attack more complex problems such as Reconnaissance Blind Chess and Autonomous Driving.\n",
    "\n",
    "In this notebook, you'll use a popular Reinforcement Learning Technique called Q-learning to train an agent to play simple games using the python library OpenAI Gym. \n",
    "\n",
    "You may refer to the following while coding:\n",
    "\n",
    "Python reference: https://bit.ly/3ajUalZ and https://bit.ly/2UgiZKa\n",
    "\n",
    "OpenAI Gym Documentation: https://gym.openai.com/docs/\n",
    "\n",
    "\n",
    "### `RL Basics:`\n",
    "Reinforcement learning is an approach used in Machine Learning where the agent is allowed to interact with the system to learn its behaviour and come up with an optimal startegy to achieve an objective. The agent models the problem as a probabilistic state machine (a graph where the transition from one node to another has a probability distribution). Nodes in the model graph are called states and a transition from one state to another is called an action. Each state transition (which is a (state, action) pair) has a corresponding reward or penalty. The goal of the RL agent is to maximise the reward. \n",
    "\n",
    "Training an RL agent from scratch requires us to model the state space and the action space. In addition, we must also come up with suitable rewards for each state transition. The RL agent estimates this reward structure and executes actions so as to maximise them. The final performance of the RL agent is heavily dependent on how the system is modelled. Luckily for us, we **do not need to get into the mathematics** of Reinforcement Learning right now, thanks to the Python library Gym.\n",
    "\n",
    "\n",
    "Gym offers many in-built RL environments which you can use to play around with. These environments are Python classes with their state spaces, action spaces and rewards pre-defined. You will use two such environments (Taxi-v3 and FrozenLake8x8-v0) to train an agent accomplish a goal. You can find the documentation for these environments here:\n",
    "\n",
    "Taxi-v3 Documentation: https://gym.openai.com/envs/Taxi-v3/  \n",
    "\n",
    "FrozenLake Documentation: https://gym.openai.com/envs/FrozenLake8x8-v0/  \n",
    "\n",
    "To create a gym environment of 'Taxi-v3' you do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 196
    },
    "colab_type": "code",
    "id": "aMtvRD03J_JO",
    "outputId": "8b4fcb9f-6e44-4231-ba73-8a90d0af657d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| :\u001b[43m \u001b[0m: : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n",
      "226\n",
      "Discrete(500)\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np  \n",
    "import random\n",
    "\n",
    "# Create an environment of Taxi-v3:\n",
    "env = gym.make('Taxi-v3').env \n",
    "env.render()\n",
    "print(env.s)\n",
    "print(env.observation_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WauFKBJeKos0"
   },
   "source": [
    "env.s is the current state of the environment.\n",
    "\n",
    "env.observation_space() returns the type and size of the observation or state space. Discrete implies that the states are discrete and not continuous. For games like Pacman, Gym uses another type of state space- Box- due to the large number of states. \n",
    "\n",
    "**Solving the Problem:** Now the goal of this game is to train the taxi to efficiently pick the passenger from the blue spot and drop them at the purple spot. The taxi can do the following actions: Move Up, Move Down, Move to the Left, Move to the Right, Pickup, Dropoff. The reward structure is such:\n",
    "1. -10 points for illegal dropoff/pickup actions\n",
    "2. +20 points if the passenger is dropped off at the correct location\n",
    "3. -1 point for every other action\n",
    "\n",
    "A paleolithic approach to this problem would be to pick an action at random and execute it. Eventually the passenger would get picked up and then dropped off at the correct location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gPwn4saqKiQN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timesteps taken: 263\n",
      "Penalties incurred: 66\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "epochs = 0\n",
    "penalty, reward = 0, 0  # Penalty records the number of times the agent hits a wall\n",
    "frames = []\n",
    "done = False # Find out the role of 'done' and complete the statement for its initial condition \n",
    "while not done: #insert condition:\n",
    "    '''\n",
    "      Enter your code here\n",
    "      The code must pick an action from the action space at random, execute it and update 'penalty' accordingly\n",
    "    '''\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, misc = env.step(action)\n",
    "    \n",
    "    if reward == -10:\n",
    "        penalty += 1\n",
    "    \n",
    "    frames.append({'frame': env.render(mode='ansi'), 'state': state, 'action': action, 'reward': reward})\n",
    "    epochs += 1\n",
    "print(\"Timesteps taken: {}\".format(epochs))\n",
    "print(\"Penalties incurred: {}\".format(penalty))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1Fwe9LT-8qn8"
   },
   "source": [
    "Run the following cell to visualise the performance of the agent. It won't come as a surprise to see that the approach is quite bad. It is so because, the agent has no memory of the past and hence learns nothing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KgvVPJ9KQWJk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :\u001b[35m\u001b[34;1m\u001b[43mG\u001b[0m\u001b[0m\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "\n",
      "Timestep: 263\n",
      "State: 85\n",
      "Action: 5\n",
      "Reward: 20\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "def print_frames(frames):\n",
    "    for i, frame in enumerate(frames):\n",
    "        clear_output(wait = True)\n",
    "        print(frame['frame'])\n",
    "        print(f\"Timestep: {i+1}\")\n",
    "        print(f\"State: {frame['state']}\")\n",
    "        print(f\"Action: {frame['action']}\")\n",
    "        print(f\"Reward: {frame['reward']}\")\n",
    "        sleep(0.1)\n",
    "print_frames(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aKcVQ8OV8-Lv"
   },
   "source": [
    "## `Enter Q-learning`\n",
    "\n",
    "A popular technique used in Reinforcement Learning is to have the agent maintain an estimate of the rewards that it would gain from executing a particular state transition (called the Q-table) which is updated after each step based on the reward obtained. The agent picks the action that yields the maximum reward at that particular state. That is, if a particular state transition ((state, action) pair) resulted in a good reward, the agent is better off repeating that action whenever that state is attained. \n",
    "\n",
    "**TASK 1:**\n",
    "\n",
    "Find out the update rule for Q-learning and implement Q-learning for the Taxi-v3 problem. Also, find out what 'exploration versus exploitation' is and use a suitable way to optimise on exploration and exploitation.\n",
    "\n",
    "You would need to set a few hyperparameters- learning rate ($\\alpha$), reward decay rate ($\\gamma$), number of episodes and exploration probability($\\epsilon$). Obtain the performance characteristics of the agent (that is, number of epochs per episode and average number of penalties per episode) for ($\\alpha$, $\\gamma$, episodes, $\\epsilon$) = (0.6, 0.9, 1000, 0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ryw4gXzjQ2iO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 100000\n",
      "Training finished.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "  Q-LEARNING ON Taxi-v3:\n",
    "  \n",
    "  Exploration vs exploitation refers to the two choices an RL agent has when prompted to make a move\n",
    "  in its learning phase. Exploration means to execute different actions toget a better idea of the \n",
    "  possible rewards at a particular state. Exploitation means to use the current knowledge of rewards,\n",
    "  instead of exploring new actions, to choose the best move possible at the current state.\n",
    "  \n",
    "  Both are necessary as an agent which only explores (i.e, random choice all the time) will not always be taking\n",
    "  the best move at a state. And an agent which only exploits without exploring will know only one path which\n",
    "  most likely is not optimal, as new actions may lead to better rewards.\n",
    "  '''\n",
    "\n",
    "q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 1.0\n",
    "gamma = 0.9\n",
    "epsilon = 0.4\n",
    "\n",
    "# For plotting metrics\n",
    "all_epochs = []\n",
    "all_penalties = []\n",
    "\n",
    "for i in range(1, 100001):\n",
    "    state = env.reset()\n",
    "\n",
    "    epochs, penalties, reward, = 0, 0, 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample() # Explore action space\n",
    "        else:\n",
    "            action = np.argmax(q_table[state]) # Exploit learned values\n",
    "\n",
    "        next_state, reward, done, info = env.step(action) \n",
    "        \n",
    "        old_value = q_table[state, action]\n",
    "        next_max = np.max(q_table[next_state])\n",
    "        \n",
    "        # The update rule of Q-learning\n",
    "        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "        q_table[state, action] = new_value\n",
    "        \n",
    "        # q_table[state, action] += alpha * (reward + gamma*next_max - old_value)\n",
    "        \n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        state = next_state\n",
    "        epochs += 1\n",
    "        \n",
    "    if i % 100 == 0:\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Episode: {i}\")\n",
    "\n",
    "print(\"Training finished.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "381\n",
      "281\n",
      "261\n",
      "241\n",
      "221\n",
      "121\n",
      "21\n",
      "1\n",
      "17\n",
      "117\n",
      "217\n",
      "237\n",
      "257\n",
      "157\n",
      "57\n",
      "77\n",
      "97\n",
      "85\n",
      "Average epochs per episode: 13.042\n",
      "Average penalties per episode: 0.0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Evaluate agent's performance after Q-learning\"\"\"\n",
    "\n",
    "total_epochs, total_penalties = 0, 0\n",
    "episodes = 1000\n",
    "\n",
    "for i in range(episodes):\n",
    "    state = env.reset()\n",
    "    epochs, penalties, reward = 0, 0, 0\n",
    "    \n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = np.argmax(q_table[state])\n",
    "        state, reward, done, info = env.step(action)\n",
    "        if i == 1:\n",
    "            print(state)\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        epochs += 1\n",
    "\n",
    "    total_penalties += penalties\n",
    "    total_epochs += epochs\n",
    "\n",
    "print(f\"Average epochs per episode: {total_epochs / episodes}\")\n",
    "print(f\"Average penalties per episode: {total_penalties / episodes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sl1WvHdREsmv"
   },
   "source": [
    "**TASK 2:**\n",
    "\n",
    "Now that you have trained the agent on Taxi-v3, try Q-learning on the FrozenLake8x8-v0 environment. After training, obtain the following performance characteristics- number of epochs per episode and average number of wins. What can you do to improve the performance of the agent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bGTlFXDrFyME"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "0\n",
      "Discrete(64)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    Displaying the Frozen Lake env.\n",
    "'''\n",
    "\n",
    "env = gym.make('FrozenLake8x8-v0').env \n",
    "env.render()\n",
    "print(env.s)\n",
    "print(env.observation_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 10000\n",
      "Training finished.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "  Q-LEARNING ON FrozenLake8x8-v0:\n",
    "  I first tried with the same parameters as above for alpha gamma and epsilon.\n",
    "  However, after some tweaking, I found that a higher gamma gave better performance.\n",
    "  \n",
    "  This is possibly because for each episode the game layout does not change, and with\n",
    "  many episodes all the state-action pairs will be explored. Gamma is the discount factor, which decides\n",
    "  whether tp give more importance to immediate rewards or later rewards. \n",
    "  Once enough exploration is done, if we give enough importance to future rewards, the table will 'learn'\n",
    "  the optimal path from the start to end.\n",
    "  \n",
    "  With gamma=1, I was able to get 100% win rate (see below next cell). \n",
    "  The number of moves taken are very large though.\n",
    "  '''\n",
    "\n",
    "q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.3\n",
    "gamma = 1.0\n",
    "epsilon = 0.5\n",
    "\n",
    "# For plotting metrics\n",
    "all_epochs = []\n",
    "all_penalties = []\n",
    "\n",
    "\n",
    "\n",
    "for i in range(1, 10001):\n",
    "    state = env.reset()\n",
    "\n",
    "    epochs, penalties, reward, = 0, 0, 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample() # Explore action space\n",
    "        else:\n",
    "            action = np.argmax(q_table[state]) # Exploit learned values\n",
    "\n",
    "        next_state, reward, done, info = env.step(action) \n",
    "        \n",
    "        old_value = q_table[state, action]\n",
    "        next_max = np.max(q_table[next_state])\n",
    "        \n",
    "        if done == True and reward == 0:\n",
    "            reward = -10\n",
    "        \n",
    "        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "        q_table[state, action] = new_value\n",
    "\n",
    "        if reward == 0:\n",
    "            penalties += 1\n",
    "\n",
    "        state = next_state\n",
    "        epochs += 1\n",
    "        \n",
    "    if i % 100 == 0:\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Episode: {i}\")\n",
    "\n",
    "print(\"Training finished.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average epochs per episode: 116.342\n",
      "Wins: 1000\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Evaluate agent's performance after Q-learning\"\"\"\n",
    "\n",
    "total_epochs, total_penalties, total_wins = 0, 0, 0\n",
    "episodes = 1000\n",
    "\n",
    "for i in range(episodes):\n",
    "    state = env.reset()\n",
    "    epochs, penalties, reward = 0, 0, 0\n",
    "    \n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = np.argmax(q_table[state])\n",
    "        state, reward, done, info = env.step(action)\n",
    "\n",
    "        if done == True and reward == 0:\n",
    "            penalties += 1\n",
    "        if done == True and reward == 1:\n",
    "            total_wins += 1\n",
    "\n",
    "        epochs += 1\n",
    "\n",
    "    total_penalties += penalties\n",
    "    total_epochs += epochs\n",
    "\n",
    "print(f\"Average epochs per episode: {total_epochs / episodes}\")\n",
    "#print(f\"Average penalties per episode: {total_penalties / episodes}\")\n",
    "print(f\"Wins: {total_wins}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Right)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFF\u001b[41mG\u001b[0m\n",
      "\n",
      "Timestep: 178\n",
      "State: 63\n",
      "Action: 2\n",
      "Reward: 1.0\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "epochs, penalties, reward = 0, 0, 0\n",
    "frames = []\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    \n",
    "    action = np.argmax(q_table[state])\n",
    "    state, reward, done, info = env.step(action)\n",
    "    \n",
    "    if reward == 0:\n",
    "        penalties += 1\n",
    "    frames.append({'frame': env.render(mode='ansi'), 'state': state, 'action': action, 'reward': reward})\n",
    "    epochs += 1\n",
    "    if epochs > 1000:\n",
    "        break\n",
    "    \n",
    "print_frames(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-7.56607133 -7.8801563  -7.19745258 -8.21138163]\n"
     ]
    }
   ],
   "source": [
    "# Ignore, Diagnostic Print\n",
    "print(q_table[53])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1331\n",
      "Episode: 1000\n",
      "Best wins out of 1000 :  1000\n",
      "Best parameters :  0.2 0.7000000000000001 0.1\n"
     ]
    }
   ],
   "source": [
    "q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.3\n",
    "gamma = 1.0\n",
    "epsilon = 0.5\n",
    "hp_c = 1\n",
    "best_ratio = 0\n",
    "best_a = -1\n",
    "best_g = -1\n",
    "best_e = -1\n",
    "best_q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "for a in [val * 0.1 for val in range(11)]:\n",
    "    for g in [val * 0.1 for val in range(11)]:\n",
    "        for e in [val * 0.1 for val in range(11)]:\n",
    "            q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "            \n",
    "            #train\n",
    "            for i in range(1, 1001):\n",
    "                state = env.reset()\n",
    "\n",
    "                epochs, penalties, reward, = 0, 0, 0\n",
    "                done = False\n",
    "    \n",
    "                while not done:\n",
    "                    if random.uniform(0, 1) < e:\n",
    "                        action = env.action_space.sample() # Explore action space\n",
    "                    else:\n",
    "                        action = np.argmax(q_table[state]) # Exploit learned values\n",
    "\n",
    "                    next_state, reward, done, info = env.step(action)\n",
    "  \n",
    "                    old_value = q_table[state, action]\n",
    "                    next_max = np.max(q_table[next_state])\n",
    "        \n",
    "                    if done == True and reward == 0:\n",
    "                        reward = -10\n",
    "        \n",
    "                    new_value = (1 - a) * old_value + a * (reward + g * next_max)\n",
    "                    q_table[state, action] = new_value\n",
    "\n",
    "                    if reward == 0:\n",
    "                        penalties += 1\n",
    "\n",
    "                    state = next_state\n",
    "                    epochs += 1\n",
    "                    if epochs > 1000:\n",
    "                        break\n",
    "        \n",
    "                if i % 100 == 0:\n",
    "                    clear_output(wait=True)\n",
    "                    print(hp_c)\n",
    "                    print(f\"Episode: {i}\")\n",
    "            hp_c += 1\n",
    "\n",
    "            #print(\"Training finished.\\n\")\n",
    "            \n",
    "            total_epochs, total_penalties, total_wins = 0, 0, 0\n",
    "            episodes = 1000\n",
    "\n",
    "            for i in range(episodes):\n",
    "                state = env.reset()\n",
    "                epochs, penalties, reward = 0, 0, 0\n",
    "    \n",
    "                done = False\n",
    "    \n",
    "                while not done:\n",
    "                    action = np.argmax(q_table[state])\n",
    "                    state, reward, done, info = env.step(action)\n",
    "\n",
    "                    if done == True and reward == 0:\n",
    "                        penalties += 1\n",
    "                    if done == True and reward == 1:\n",
    "                        total_wins += 1\n",
    "\n",
    "                    epochs += 1\n",
    "                    if epochs>1000:\n",
    "                        break\n",
    "\n",
    "                total_penalties += penalties\n",
    "                total_epochs += epochs\n",
    "            \n",
    "            if total_wins > best_wins:\n",
    "                best_wins = total_wins\n",
    "                best_q_table = q_table\n",
    "                best_a, best_g, best_e = a,g,e\n",
    "                best_ratio = total_wins/(total_epochs/episodes)\n",
    "            elif total_wins == best_wins and total_wins/(total_epochs/episodes) > best_ratio:\n",
    "                best_wins = total_wins\n",
    "                best_q_table = q_table\n",
    "                best_a, best_g, best_e = a,g,e\n",
    "                best_ratio = total_wins/(total_epochs/episodes) \n",
    "                \n",
    "print(\"Best wins out of 1000 : \", best_wins)\n",
    "print(\"Best parameters : \", best_a, best_g, best_e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1000\n",
      "Training finished.\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-e54323933af5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gym/envs/toy_text/discrete.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, a)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlastaction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"prob\"\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = best_a\n",
    "gamma = best_g\n",
    "epsilon = best_e\n",
    "\n",
    "# For plotting metrics\n",
    "all_epochs = []\n",
    "all_penalties = []\n",
    "\n",
    "\n",
    "\n",
    "for i in range(1, 1001):\n",
    "    state = env.reset()\n",
    "\n",
    "    epochs, penalties, reward, = 0, 0, 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample() # Explore action space\n",
    "        else:\n",
    "            action = np.argmax(q_table[state]) # Exploit learned values\n",
    "\n",
    "        next_state, reward, done, info = env.step(action) \n",
    "        \n",
    "        old_value = q_table[state, action]\n",
    "        next_max = np.max(q_table[next_state])\n",
    "        \n",
    "        if done == True and reward == 0:\n",
    "            reward = -10\n",
    "        \n",
    "        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "        q_table[state, action] = new_value\n",
    "\n",
    "        if reward == 0:\n",
    "            penalties += 1\n",
    "\n",
    "        state = next_state\n",
    "        epochs += 1\n",
    "        \n",
    "    if i % 100 == 0:\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Episode: {i}\")\n",
    "\n",
    "print(\"Training finished.\\n\")\n",
    "\n",
    "total_epochs, total_penalties, total_wins = 0, 0, 0\n",
    "episodes = 1000\n",
    "\n",
    "for i in range(episodes):\n",
    "    state = env.reset()\n",
    "    epochs, penalties, reward = 0, 0, 0\n",
    "    \n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = np.argmax(q_table[state])\n",
    "        state, reward, done, info = env.step(action)\n",
    "\n",
    "        if done == True and reward == 0:\n",
    "            penalties += 1\n",
    "        if done == True and reward == 1:\n",
    "            total_wins += 1\n",
    "\n",
    "        epochs += 1\n",
    "\n",
    "    total_penalties += penalties\n",
    "    total_epochs += epochs\n",
    "\n",
    "print(f\"Average epochs per episode: {total_epochs / episodes}\")\n",
    "#print(f\"Average penalties per episode: {total_penalties / episodes}\")\n",
    "print(f\"Wins: {total_wins}\")\n",
    "\n",
    "state = env.reset()\n",
    "epochs, penalties, reward = 0, 0, 0\n",
    "frames = []\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    \n",
    "    action = np.argmax(q_table[state])\n",
    "    state, reward, done, info = env.step(action)\n",
    "    \n",
    "    if reward == 0:\n",
    "        penalties += 1\n",
    "    frames.append({'frame': env.render(mode='ansi'), 'state': state, 'action': action, 'reward': reward})\n",
    "    epochs += 1\n",
    "    if epochs > 1000:\n",
    "        break\n",
    "    \n",
    "print_frames(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LITRSgg9I33Z"
   },
   "source": [
    "## `Further Motivation`\n",
    "\n",
    "In case you are curious about the mathematics of Reinforcement Learning, you check the following resources out:\n",
    "\n",
    "RL Lectures by Dr. David Silver: http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html\n",
    "\n",
    "Medium Blog Post on RL techniques: https://medium.com/@jonathan_hui/rl-introduction-to-deep-reinforcement-learning-35c25e04c199\n",
    "\n",
    "Deep Neural Networks (useful for Deep RL): http://cs231n.github.io/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bjpYZfLKIOtJ"
   },
   "source": [
    "If you are facing any issue or difficulty, you may PM (on WhatsApp):\n",
    "\n",
    "Arjun: 97392 19819"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "RLGames_ps.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
